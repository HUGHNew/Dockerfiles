ARG BASE_IMAGE
FROM ${BASE_IMAGE:-root} AS base
ARG _GID
ARG _GRP=docker
ARG VLLM_VER
SHELL ["/bin/bash", "-c"]

ENV PYTHONPATH=.:${PYTHONPATH}
RUN groupadd -g ${_GID} ${_GRP} && \
    chown -R :${_GID} /opt/micromamba && \
    chmod -R g+w /opt/micromamba && \
    source /etc/profile.d/mamba.sh && \
    micromamba install -y python=3.12
ENV PATH=$PATH:/opt/micromamba/bin
RUN pip install vllm==${VLLM_VER} --extra-index-url https://download.pytorch.org/whl/cu128 --no-cache-dir && \
    pip cache purge
# pip install flash-attn~=2.0 --no-build-isolation --no-cache-dir # depends on cuda

## public ends ##